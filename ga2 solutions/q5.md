# Problem Samjhiye problem statement

**Company:**
eShopCo

**Situation:** 
Har storefront se latency data stream ho raha hai FastAPI service mein. Product managers ko ek 
aisa serverless endpoint chahiye jo woh apne dashboards se call kar sakein check
karne ke liye ki deployment target latency ke under hai ya nahi.

### step 1:vercel.json

```json
{
  "builds": [
    {
      "src": "api/latency-check.py",
      "use": "@vercel/python"
    }
  ],
  "routes": [
    {
      "src": "/(.*)",
      "dest": "/api/latency-check.py"
    }
  ]
}
```

**Explanation**
- builds - Tells Vercel to use Python for our latency-check.py file
- routes - Redirects ALL incoming requests to our Python function
- @vercel/python - This is Vercel's Python runtime environment
### step2:Requirements.txt
```text
fastapi
pydantic
```
**Yeh File Kya Karti Hai:**
- Vercel ko batati hai ki "Bhai, mere app ko yeh Python packages chahiye"
- fastapi - Web framework jo API banane mein help karega
- pydantic - Data validation ke liye (JSON request/response check karta hai)
  
**Kyun Important Hai:**
- Vercel serverless mein tum pip install nahi kar sakte
- Isliye requirements.txt mein likhna padta hai
- Deployment time pe Vercel automatically in packages ko install kar lega

**Example:**
Jaise tumhe koi aur package chahiye ho, toh simply add kar do:
```text
fastapi
pydantic
uvicorn
```
### step 3:api/latency-check.py
```python
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List
import statistics
from fastapi.middleware.cors import CORSMiddleware
import os

app = FastAPI()

# CORS Setup - Yeh important hai
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Sabhi websites se request allow karo
    allow_methods=["POST"],  # Sirf POST requests allow karo
    allow_headers=["*"],  # Sabhi headers allow karo
)


class LatencyRequest(BaseModel):
    regions: List[str]
    threshold_ms: int

class RegionMetrics(BaseModel):
    region: str
    avg_latency: float
    p95_latency: float
    avg_uptime: float
    breaches: int

class LatencyResponse(BaseModel):
    regions: List[RegionMetrics]

# Mock telemetry data - temporary testing data
def load_telemetry_data():
    return [
        {"region": "apac", "latency": 120, "uptime": 99.9},
        {"region": "apac", "latency": 180, "uptime": 99.8},
        {"region": "amer", "latency": 140, "uptime": 99.7},
        {"region": "amer", "latency": 200, "uptime": 99.6},
    ]

@app.post("/api/latency-check")
async def check_latency(request: LatencyRequest):
    telemetry_data = load_telemetry_data()
    
    region_metrics = []
    
    for region in request.regions:
        # Current region ki data filter karo
        region_data = [item for item in telemetry_data if item["region"] == region]
        
        # Calculations karo:
        latencies = [item["latency"] for item in region_data]
        uptimes = [item["uptime"] for item in region_data]
        
        avg_latency = statistics.mean(latencies)
        p95_latency = statistics.quantiles(latencies, n=20)[18]  # 95th percentile
        avg_uptime = statistics.mean(uptimes)
        breaches = sum(1 for latency in latencies if latency > request.threshold_ms)
        
        region_metrics.append(RegionMetrics(
            region=region,
            avg_latency=round(avg_latency, 2),
            p95_latency=round(p95_latency, 2),
            avg_uptime=round(avg_uptime, 2),
            breaches=breaches
        ))
    
    return LatencyResponse(regions=region_metrics)
```
**Kya Ho Raha Hai Yahan:**
- Imports - FastAPI, data validation, statistics calculation ke liye
- CORS Setup - Yeh woh cheez hai jo different websites se requests allow karti hai
 
(CORS Kyun Important Hai?
  
Tumhare dashboard se direct API call karne ke liye
Bina CORS ke browser block kar dega requests)
- allow_origins=["*"] = kisi bhi website se API call karne do
- allow_methods=["POST"] = sirf POST requests allow karo

**classes expalination:**
* **LatencyRequest** - Incoming JSON ki shape define karta hai:
```json
{
  "regions": ["apac", "amer"],
  "threshold_ms": 154
}
```
* **RegionMetrics** - Har region ka result dikhata hai:
  * ```region``` - "apac" ya "amer"
  * ```avg_latency``` - Average latency
  *  ```p95_latency``` - 95th percentile latency
  * ```avg_uptime``` - Average uptime
  * ```breaches``` - Kitne records threshold se upar the
* **LatencyResponse** - Final output format:
```json
{
  "regions": [
    { ...region 1 data... },
    { ...region 2 data... }
  ]
}
```
**Kaun Kya Kaam Karta Hai:**
* Request Model - Validate karta hai ki sahi data aaya hai
* Response Model - Ensure karta hai ki sahi format mein response jaye

**Ab dekhte hain actual calculation wala hissa:**
* ```statistics.mean()``` - Average nikalta hai
* ```statistics.quantiles()``` - 95th percentile nikalta hai
* ```sum()``` - Count karta hai kitne breaches hue

 ### step 3 :Deployment Process - Practical Steps
 **Step 1: Files Create Karo**
 ```text
eshopco-latency/
├── api/
│   └── latency-check.py
├── requirements.txt
└── vercel.json
```
**Step 2: Terminal/Command Prompt Mein Jaao**
```bash
# Project folder mein jaao
cd eshopco-latency

# Git initialize karo
git init
git add .
git commit -m "First deployment"
```
**Step 3: Vercel Deployment**
```bash
# Pehli baar deployment
npx vercel

# Ye questions puchenga:
? Set up and deploy? (Y/n) ➜ Y
? Which scope? ➜ Tumhara account choose karo
? Link to existing project? (y/N) ➜ N
? Project name? ➜ yaha apna favorite naam do (jaise: eshopco-latency)
? In which directory? ➜ . (current directory)
```
**Step 4: Production Deployment**
```bash
# Final production deploy
npx vercel --prod
```
**Step 5: Test Karo**
Deployment ke baad mil jayega tumhara URL:

```https://eshopco-latency-tumhara-account.vercel.app/api/latency-check```

Test karne ke liye:
```bash
curl -X POST https://your-url.vercel.app/api/latency-check \
  -H "Content-Type: application/json" \
  -d '{"regions":["apac","amer"],"threshold_ms":154}'
```







